# LLM

## RAG 是什么

RAG（Retrieval-Augmented Generation）是一种结合了检索和生成两种方法的自然语言处理模型。它首先使用一个检索模型从大规模语料库中检索与输入相关的文档，然后将这些文档作为上下文输入到一个生成模型中，生成最终的输出。

RAG 模型由两个主要部分组成：检索模型和生成模型。检索模型负责从大规模语料库中检索与输入相关的文档，生成模型则根据检索到的文档和输入生成最终的输出。
检索模型通常使用一种称为“注意力机制”的技术，通过计算输入和文档之间的相似度来选择最相关的文档。生成模型则使用一种称为“解码器”的技术，根据检索到的文档和输入生成最终的输出。

RAG 模型的一个主要优点是它可以处理比生成模型更大的上下文信息，因为它可以检索到与输入相关的文档。这使得 RAG 模型在处理复杂任务时更加有效，例如问答、文本摘要和机器翻译等。

优点：

1. **更丰富的上下文信息**：RAG 模型可以检索到与输入相关的文档，从而提供更丰富的上下文信息，这对于处理复杂任务非常有用。
2. **更好的性能**：RAG 模型通常比生成模型具有更好的性能，因为它可以更好地处理复杂任务。
3. **可解释性**：RAG 模型可以提供更清晰的解释，因为它可以显示检索到的文档和生成模型的输出。

缺点：

1. **计算成本高**：RAG 模型需要同时运行检索模型和生成模型，这会增加计算成本。
2. **需要大量的训练数据**：RAG 模型需要大量的训练数据来训练检索模型和生成模型，这可能会增加训练成本。
3. **需要大量的存储空间**：RAG 模型需要存储大量的文档，这可能会增加存储空间的需求。

​ 当前局限：

1. ​ 检索噪声：不相关文档可能误导生成结果。
2. ​ 系统复杂度：需协调检索器、生成器等多组件，优化超参数（如分块大小、Top-K 值）

## MCP 是什么

## Function Call 是什么

Function Call（函数调用）​ 是一种在人工智能模型中实现交互的技术，它允许模型根据用户的自然语言输入，识别其意图并触发预先设定的外部函数或工具，从而执行特定的任务

- Function Call 的核心机制

  1. 模型首先通过自然语言处理技术，如词嵌入、序列标注等，将用户的自然语言输入转化为模型可以理解的格式。
  2. 模型根据输入的上下文信息，识别用户的意图，并选择相应的函数或工具。
  3. 模型将用户的输入作为参数传递给选定的函数或工具，并执行相应的任务。
  4. 模型将函数或工具的输出返回给用户，以实现与用户的交互。

- Function Call 的优势

  1. 提高模型的交互能力：Function Call 可以使模型根据用户的自然语言输入，触发预先设定的外部函数或工具，从而实现与用户的交互。
  2. 提高模型的实用性：Function Call 可以使模型根据用户的实际需求，执行特定的任务，从而提高模型的实用性。
  3. 提高模型的可扩展性：Function Call 可以使模型根据用户的需求，动态地添加或删除函数或工具，从而提高模型的可扩展性。

- Function Call 的应用场景

  1. 问答系统：Function Call 可以使问答系统根据用户的自然语言输入，触发预先设定的外部知识库或搜索引擎，从而回答用户的问题。
  2. 语音助手：Function Call 可以使语音助手根据用户的语音输入，触发预先设定的外部函数或工具，从而执行特定的任务，如播放音乐、设置闹钟等。

## In-ContextLearning 是什么

In-Context Learning（上下文学习）是一种在人工智能模型中实现交互的技术，它允许模型根据用户的自然语言输入，识别其意图并触发预先设定的外部函数或工具，从而执行特定的任务。

- In-Context Learning 的核心机制

  1. 模型首先通过自然语言处理技术，如词嵌入、序列标注等，将用户的自然语言输入转化为模型可以理解的格式。
  2. 模型根据输入的上下文信息，识别用户的意图，并选择相应的函数或工具。
  3. 模型将用户的输入作为参数传递给选定的函数或工具，并执行相应的任务。
  4. 模型将函数或工具的输出返回给用户，以实现与用户的交互。

- In-Context Learning 的优势

  1. 提高模型的交互能力：In-Context Learning 可以使模型根据用户的自然语言输入，触发预先设定的外部函数或工具，从而实现与用户的交互。
  2. 提高模型的实用性：In-Context Learning 可以使模型根据用户的实际需求，执行特定的任务，从而提高模型的实用性。
  3. 提高模型的可扩展性：In-Context Learning 可以使模型根据用户的需求，动态地添加或删除函数或工具，从而提高模型的可扩展性。

- In-Context Learning 的应用场景

  1. 问答系统：In-Context Learning 可以使问答系统根据用户的自然语言输入，触发预先设定的外部知识库或搜索引擎，从而回答用户的问题。

## File Match 是什么

File Match 是一种在人工智能模型中实现交互的技术，它允许模型根据用户的自然语言输入，识别其意图并触发预先设定的外部函数或工具，从而执行特定的任务。

- File Match 的核心机制

  1. 模型首先通过自然语言处理技术，如词嵌入、序列标注等，将用户的自然语言输入转化为模型可以理解的格式。
  2. 模型根据输入的上下文信息，识别用户的意图，并选择相应的函数或工具。
  3. 模型将用户的输入作为参数传递给选定的函数或工具，并执行相应的任务。
  4. 模型将函数或工具的输出返回给用户，以实现与用户的交互。

- File Match 的优势

  1. 提高模型的交互能力：File Match 可以使模型根据用户的自然语言输入，触发预先设定的外部函数或工具，从而实现与用户的交互。
  2. 提高模型的实用性：File Match 可以使模型根据用户的实际需求，执行特定的任务，从而提高模型的实用性。
  3. 提高模型的可扩展性：File Match 可以使模型根据用户的需求，动态地添加或删除函数或工具，从而提高模型的可扩展性。

- File Match 的应用场景

  1. 问答系统：File Match 可以使问答系统根据用户的自然语言输入，触发预先设定的外部知识库或搜索引擎，从而回答用户的问题。
